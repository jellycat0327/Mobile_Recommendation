{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### file path\n",
    "# input \n",
    "path_df_D = \"tianchi_fresh_comp_train_user.csv\"\n",
    "\n",
    "path_df_part_1 = \"mobile/df_part_1.csv\"\n",
    "path_df_part_2 = \"mobile/df_part_2.csv\"\n",
    "path_df_part_3 = \"mobile/df_part_3.csv\"\n",
    "\n",
    "path_df_part_1_tar = \"mobile/df_part_1_tar.csv\"\n",
    "path_df_part_2_tar = \"mobile/df_part_2_tar.csv\"\n",
    "\n",
    "path_df_part_1_uic_label = \"mobile/df_part_1_uic_label.csv\"\n",
    "path_df_part_2_uic_label = \"mobile/df_part_2_uic_label.csv\"\n",
    "path_df_part_3_uic       = \"mobile/df_part_3_uic.csv\"\n",
    "\n",
    "# output\n",
    "path_df_part_1_U   = \"mobile/feature/df_part_1_U.csv\"  \n",
    "path_df_part_1_I   = \"mobile/feature/df_part_1_I.csv\"\n",
    "path_df_part_1_C   = \"mobile/feature/df_part_1_C.csv\"\n",
    "path_df_part_1_IC  = \"mobile/feature/df_part_1_IC.csv\"\n",
    "path_df_part_1_UI  = \"mobile/feature/df_part_1_UI.csv\"\n",
    "path_df_part_1_UC  = \"mobile/feature/df_part_1_UC.csv\"\n",
    "\n",
    "path_df_part_2_U   = \"mobile/feature/df_part_2_U.csv\"  \n",
    "path_df_part_2_I   = \"mobile/feature/df_part_2_I.csv\"\n",
    "path_df_part_2_C   = \"mobile/feature/df_part_2_C.csv\"\n",
    "path_df_part_2_IC  = \"mobile/feature/df_part_2_IC.csv\"\n",
    "path_df_part_2_UI  = \"mobile/feature/df_part_2_UI.csv\"\n",
    "path_df_part_2_UC  = \"mobile/feature/df_part_2_UC.csv\"\n",
    "\n",
    "path_df_part_3_U   = \"mobile/feature/df_part_3_U.csv\"  \n",
    "path_df_part_3_I   = \"mobile/feature/df_part_3_I.csv\"\n",
    "path_df_part_3_C   = \"mobile/feature/df_part_3_C.csv\"\n",
    "path_df_part_3_IC  = \"mobile/feature/df_part_3_IC.csv\"\n",
    "path_df_part_3_UI  = \"mobile/feature/df_part_3_UI.csv\"\n",
    "path_df_part_3_UC  = \"mobile/feature/df_part_3_UC.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# loading data\n",
    "path_df = open(path_df_part_2, 'r')\n",
    "try:\n",
    "    df_part_2 = pd.read_csv(path_df, index_col = False, parse_dates = [0])\n",
    "    df_part_2.columns = ['time','user_id','item_id','behavior_type','item_category']\n",
    "finally:\n",
    "    path_df.close()\n",
    "\n",
    "# u_b_count_in_6\n",
    "df_part_2['cumcount'] = df_part_2.groupby(['user_id', 'behavior_type']).cumcount()\n",
    "df_part_2_u_b_count_in_6 = df_part_2.drop_duplicates(['user_id','behavior_type'], 'last')[['user_id','behavior_type','cumcount']]\n",
    "df_part_2_u_b_count_in_6 = pd.get_dummies(df_part_2_u_b_count_in_6['behavior_type']).join(df_part_2_u_b_count_in_6[['user_id','cumcount']])\n",
    "df_part_2_u_b_count_in_6.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_u_b_count_in_6['u_b1_count_in_6'] = df_part_2_u_b_count_in_6['behavior_type_1'] * (df_part_2_u_b_count_in_6['cumcount']+1)\n",
    "df_part_2_u_b_count_in_6['u_b2_count_in_6'] = df_part_2_u_b_count_in_6['behavior_type_2'] * (df_part_2_u_b_count_in_6['cumcount']+1)\n",
    "df_part_2_u_b_count_in_6['u_b3_count_in_6'] = df_part_2_u_b_count_in_6['behavior_type_3'] * (df_part_2_u_b_count_in_6['cumcount']+1)\n",
    "df_part_2_u_b_count_in_6['u_b4_count_in_6'] = df_part_2_u_b_count_in_6['behavior_type_4'] * (df_part_2_u_b_count_in_6['cumcount']+1)\n",
    "df_part_2_u_b_count_in_6 = df_part_2_u_b_count_in_6.groupby('user_id').agg({'u_b1_count_in_6': np.sum,\n",
    "                                                                            'u_b2_count_in_6': np.sum,\n",
    "                                                                            'u_b3_count_in_6': np.sum,\n",
    "                                                                            'u_b4_count_in_6': np.sum})\n",
    "df_part_2_u_b_count_in_6.reset_index(inplace = True)\n",
    "df_part_2_u_b_count_in_6['u_b_count_in_6'] = df_part_2_u_b_count_in_6[['u_b1_count_in_6',\n",
    "                                                                       'u_b2_count_in_6',\n",
    "                                                                       'u_b3_count_in_6',\n",
    "                                                                       'u_b4_count_in_6']].apply(lambda x: x.sum(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# u_b_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')]\n",
    "df_part_2_in_3['cumcount'] = df_part_2_in_3.groupby(['user_id', 'behavior_type']).cumcount()\n",
    "df_part_2_u_b_count_in_3 = df_part_2.drop_duplicates(['user_id','behavior_type'], 'last')[['user_id','behavior_type','cumcount']]\n",
    "df_part_2_u_b_count_in_3 = pd.get_dummies(df_part_2_u_b_count_in_3['behavior_type']).join(df_part_2_u_b_count_in_3[['user_id','cumcount']])\n",
    "df_part_2_u_b_count_in_3.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_u_b_count_in_3['u_b1_count_in_3'] = df_part_2_u_b_count_in_3['behavior_type_1'] * (df_part_2_u_b_count_in_3['cumcount']+1)\n",
    "df_part_2_u_b_count_in_3['u_b2_count_in_3'] = df_part_2_u_b_count_in_3['behavior_type_2'] * (df_part_2_u_b_count_in_3['cumcount']+1)\n",
    "df_part_2_u_b_count_in_3['u_b3_count_in_3'] = df_part_2_u_b_count_in_3['behavior_type_3'] * (df_part_2_u_b_count_in_3['cumcount']+1)\n",
    "df_part_2_u_b_count_in_3['u_b4_count_in_3'] = df_part_2_u_b_count_in_3['behavior_type_4'] * (df_part_2_u_b_count_in_3['cumcount']+1)\n",
    "df_part_2_u_b_count_in_3 = df_part_2_u_b_count_in_3.groupby('user_id').agg({'u_b1_count_in_3': np.sum,\n",
    "                                                                            'u_b2_count_in_3': np.sum,\n",
    "                                                                            'u_b3_count_in_3': np.sum,\n",
    "                                                                            'u_b4_count_in_3': np.sum})\n",
    "df_part_2_u_b_count_in_3.reset_index(inplace = True)\n",
    "df_part_2_u_b_count_in_3['u_b_count_in_3'] = df_part_2_u_b_count_in_3[['u_b1_count_in_3',\n",
    "                                                                       'u_b2_count_in_3',\n",
    "                                                                       'u_b3_count_in_3',\n",
    "                                                                       'u_b4_count_in_3']].apply(lambda x: x.sum(), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# u_b_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')]\n",
    "df_part_2_in_1['cumcount'] = df_part_2_in_1.groupby(['user_id', 'behavior_type']).cumcount()\n",
    "df_part_2_u_b_count_in_1 = df_part_2_in_1.drop_duplicates(['user_id','behavior_type'], 'last')[['user_id','behavior_type','cumcount']]\n",
    "df_part_2_u_b_count_in_1 = pd.get_dummies(df_part_2_u_b_count_in_1['behavior_type']).join(df_part_2_u_b_count_in_1[['user_id','cumcount']])\n",
    "df_part_2_u_b_count_in_1.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_u_b_count_in_1['u_b1_count_in_1'] = df_part_2_u_b_count_in_1['behavior_type_1'] * (df_part_2_u_b_count_in_1['cumcount']+1)\n",
    "df_part_2_u_b_count_in_1['u_b2_count_in_1'] = df_part_2_u_b_count_in_1['behavior_type_2'] * (df_part_2_u_b_count_in_1['cumcount']+1)\n",
    "df_part_2_u_b_count_in_1['u_b3_count_in_1'] = df_part_2_u_b_count_in_1['behavior_type_3'] * (df_part_2_u_b_count_in_1['cumcount']+1)\n",
    "df_part_2_u_b_count_in_1['u_b4_count_in_1'] = df_part_2_u_b_count_in_1['behavior_type_4'] * (df_part_2_u_b_count_in_1['cumcount']+1)\n",
    "df_part_2_u_b_count_in_1 = df_part_2_u_b_count_in_1.groupby('user_id').agg({'u_b1_count_in_1': np.sum,\n",
    "                                                                            'u_b2_count_in_1': np.sum,\n",
    "                                                                            'u_b3_count_in_1': np.sum,\n",
    "                                                                            'u_b4_count_in_1': np.sum})\n",
    "df_part_2_u_b_count_in_1.reset_index(inplace = True)\n",
    "df_part_2_u_b_count_in_1['u_b_count_in_1']  = df_part_2_u_b_count_in_1[['u_b1_count_in_1',\n",
    "                                                                        'u_b2_count_in_1',\n",
    "                                                                        'u_b3_count_in_1',\n",
    "                                                                        'u_b4_count_in_1']].apply(lambda x: x.sum(), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the result of count_in_6, count_in_3, count_in_1\n",
    "\n",
    "df_part_2_u_b_count = pd.merge(df_part_2_u_b_count_in_6, \n",
    "                               df_part_2_u_b_count_in_3, on = ['user_id'], how = 'left').fillna(0)\n",
    "df_part_2_u_b_count = pd.merge(df_part_2_u_b_count, \n",
    "                               df_part_2_u_b_count_in_1, on = ['user_id'], how = 'left').fillna(0)\n",
    "                                    \n",
    "df_part_2_u_b_count[['u_b1_count_in_6',\n",
    "                     'u_b2_count_in_6',\n",
    "                     'u_b3_count_in_6',\n",
    "                     'u_b4_count_in_6',\n",
    "                      'u_b_count_in_6',\n",
    "                     'u_b1_count_in_3',\n",
    "                     'u_b2_count_in_3',\n",
    "                     'u_b3_count_in_3',\n",
    "                     'u_b4_count_in_3',\n",
    "                      'u_b_count_in_3',\n",
    "                     'u_b1_count_in_1',\n",
    "                     'u_b2_count_in_1',\n",
    "                     'u_b3_count_in_1',\n",
    "                     'u_b4_count_in_1',\n",
    "                      'u_b_count_in_1']] = df_part_2_u_b_count[['u_b1_count_in_6',\n",
    "                                                                'u_b2_count_in_6',\n",
    "                                                                'u_b3_count_in_6',\n",
    "                                                                'u_b4_count_in_6',\n",
    "                                                                 'u_b_count_in_6',\n",
    "                                                                'u_b1_count_in_3',\n",
    "                                                                'u_b2_count_in_3',\n",
    "                                                                'u_b3_count_in_3',\n",
    "                                                                'u_b4_count_in_3',\n",
    "                                                                 'u_b_count_in_3',\n",
    "                                                                'u_b1_count_in_1',\n",
    "                                                                'u_b2_count_in_1',\n",
    "                                                                'u_b3_count_in_1',\n",
    "                                                                'u_b4_count_in_1',\n",
    "                                                                 'u_b_count_in_1']].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_b4_rate\n",
    "df_part_2_u_b_count['u_b4_rate'] = df_part_2_u_b_count['u_b4_count_in_6'] / df_part_2_u_b_count['u_b_count_in_6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_b4_diff_time\n",
    "df_part_2 = df_part_2.sort_values(by = ['user_id', 'time'])\n",
    "df_part_2_u_b4_time = df_part_2[df_part_2['behavior_type'] == 4].drop_duplicates(['user_id'],'first')[['user_id','time']]\n",
    "df_part_2_u_b4_time.columns = ['user_id','b4_first_time']\n",
    "df_part_2_u_b_time = df_part_2.drop_duplicates(['user_id'],'first')[['user_id','time']]\n",
    "df_part_2_u_b_time.columns = ['user_id','b_first_time']\n",
    "df_part_2_u_b_b4_time = pd.merge(df_part_2_u_b_time, df_part_2_u_b4_time, on = ['user_id'])\n",
    "df_part_2_u_b_b4_time['u_b4_diff_time'] = df_part_2_u_b_b4_time['b4_first_time'] - df_part_2_u_b_b4_time['b_first_time']\n",
    "df_part_2_u_b_b4_time = df_part_2_u_b_b4_time[['user_id', 'u_b4_diff_time']]\n",
    "df_part_2_u_b_b4_time['u_b4_diff_hours'] = df_part_2_u_b_b4_time['u_b4_diff_time'].apply(lambda x: x.days * 24 + x.seconds//3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating feature set U\n",
    "f_U_part_2 = pd.merge(df_part_2_u_b_count, \n",
    "                      df_part_2_u_b_b4_time, \n",
    "                      on = ['user_id'], how = 'left')[['user_id',\n",
    "                                                       'u_b1_count_in_6', \n",
    "                                                       'u_b2_count_in_6', \n",
    "                                                       'u_b3_count_in_6', \n",
    "                                                       'u_b4_count_in_6', \n",
    "                                                       'u_b_count_in_6',\n",
    "                                                       'u_b1_count_in_3',\n",
    "                                                       'u_b2_count_in_3', \n",
    "                                                       'u_b3_count_in_3',\n",
    "                                                       'u_b4_count_in_3', \n",
    "                                                       'u_b_count_in_3',\n",
    "                                                       'u_b1_count_in_1',\n",
    "                                                       'u_b2_count_in_1', \n",
    "                                                       'u_b3_count_in_1',\n",
    "                                                       'u_b4_count_in_1', \n",
    "                                                       'u_b_count_in_1', \n",
    "                                                       'u_b4_rate', \n",
    "                                                       'u_b4_diff_hours']]\n",
    "# write to csv file\n",
    "f_U_part_2 = f_U_part_2.round({'u_b4_rate': 3})\n",
    "f_U_part_2.to_csv(path_df_part_2_U, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "path_df = open(path_df_part_2, 'r')\n",
    "try:\n",
    "    df_part_2 = pd.read_csv(path_df, index_col = False, parse_dates = [0])\n",
    "    df_part_2.columns = ['time','user_id','item_id','behavior_type','item_category']\n",
    "finally:\n",
    "    path_df.close()\n",
    "\n",
    "# i_u_count_in_6\n",
    "df_part_2_in_6 = df_part_2.drop_duplicates(['item_id', 'user_id'])\n",
    "df_part_2_in_6['i_u_count_in_6'] = df_part_2_in_6.groupby('item_id').cumcount() + 1\n",
    "df_part_2_i_u_count_in_6 = df_part_2_in_6.drop_duplicates(['item_id'], 'last')[['item_id', 'i_u_count_in_6']]\n",
    "\n",
    "# i_u_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')].drop_duplicates(['item_id', 'user_id'])\n",
    "df_part_2_in_3['i_u_count_in_3'] = df_part_2_in_3.groupby('item_id').cumcount() + 1\n",
    "df_part_2_i_u_count_in_3 = df_part_2_in_3.drop_duplicates(['item_id'], 'last')[['item_id', 'i_u_count_in_3']]\n",
    "\n",
    "# i_u_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')].drop_duplicates(['item_id', 'user_id'])\n",
    "df_part_2_in_1['i_u_count_in_1'] = df_part_2_in_1.groupby('item_id').cumcount() + 1\n",
    "df_part_2_i_u_count_in_1 = df_part_2_in_1.drop_duplicates(['item_id'], 'last')[['item_id', 'i_u_count_in_1']]\n",
    "\n",
    "# merge for generation of i_u_count\n",
    "df_part_2_i_u_count = pd.merge(df_part_2_i_u_count_in_6, \n",
    "                               df_part_2_i_u_count_in_3,\n",
    "                               on=['item_id'],how='left').fillna(0)\n",
    "df_part_2_i_u_count = pd.merge(df_part_2_i_u_count, \n",
    "                               df_part_2_i_u_count_in_1,\n",
    "                               on=['item_id'],how='left').fillna(0)\n",
    "df_part_2_i_u_count[['i_u_count_in_6',\n",
    "                     'i_u_count_in_3',\n",
    "                     'i_u_count_in_1']] = df_part_2_i_u_count[['i_u_count_in_6',\n",
    "                                                               'i_u_count_in_3',\n",
    "                                                               'i_u_count_in_1']].astype(int)\n",
    "\n",
    "# i_b_count_in_6\n",
    "df_part_2['cumcount'] = df_part_2.groupby(['item_id', 'behavior_type']).cumcount()\n",
    "df_part_2_i_b_count_in_6 = df_part_2.drop_duplicates(['item_id','behavior_type'], 'last')[['item_id','behavior_type','cumcount']]\n",
    "df_part_2_i_b_count_in_6 = pd.get_dummies(df_part_2_i_b_count_in_6['behavior_type']).join(df_part_2_i_b_count_in_6[['item_id','cumcount']])\n",
    "df_part_2_i_b_count_in_6.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_i_b_count_in_6['i_b1_count_in_6'] = df_part_2_i_b_count_in_6['behavior_type_1'] * (df_part_2_i_b_count_in_6['cumcount']+1)\n",
    "df_part_2_i_b_count_in_6['i_b2_count_in_6'] = df_part_2_i_b_count_in_6['behavior_type_2'] * (df_part_2_i_b_count_in_6['cumcount']+1)\n",
    "df_part_2_i_b_count_in_6['i_b3_count_in_6'] = df_part_2_i_b_count_in_6['behavior_type_3'] * (df_part_2_i_b_count_in_6['cumcount']+1)\n",
    "df_part_2_i_b_count_in_6['i_b4_count_in_6'] = df_part_2_i_b_count_in_6['behavior_type_4'] * (df_part_2_i_b_count_in_6['cumcount']+1)\n",
    "df_part_2_i_b_count_in_6 = df_part_2_i_b_count_in_6[['item_id', \n",
    "                                                     'i_b1_count_in_6', \n",
    "                                                     'i_b2_count_in_6', \n",
    "                                                     'i_b3_count_in_6',\n",
    "                                                     'i_b4_count_in_6']]\n",
    "df_part_2_i_b_count_in_6 = df_part_2_i_b_count_in_6.groupby('item_id').agg({'i_b1_count_in_6': np.sum,\n",
    "                                                                            'i_b2_count_in_6': np.sum,\n",
    "                                                                            'i_b3_count_in_6': np.sum,\n",
    "                                                                            'i_b4_count_in_6': np.sum})\n",
    "df_part_2_i_b_count_in_6.reset_index(inplace = True)\n",
    "df_part_2_i_b_count_in_6['i_b_count_in_6'] = df_part_2_i_b_count_in_6['i_b1_count_in_6'] + \\\n",
    "                                             df_part_2_i_b_count_in_6['i_b2_count_in_6'] + \\\n",
    "                                             df_part_2_i_b_count_in_6['i_b3_count_in_6'] + \\\n",
    "                                             df_part_2_i_b_count_in_6['i_b4_count_in_6']\n",
    "\n",
    "# i_b_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')]\n",
    "df_part_2_in_3['cumcount'] = df_part_2_in_3.groupby(['item_id', 'behavior_type']).cumcount()\n",
    "df_part_2_i_b_count_in_3 = df_part_2.drop_duplicates(['item_id','behavior_type'], 'last')[['item_id','behavior_type','cumcount']]\n",
    "df_part_2_i_b_count_in_3 = pd.get_dummies(df_part_2_i_b_count_in_3['behavior_type']).join(df_part_2_i_b_count_in_3[['item_id','cumcount']])\n",
    "df_part_2_i_b_count_in_3.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_i_b_count_in_3['i_b1_count_in_3'] = df_part_2_i_b_count_in_3['behavior_type_1'] * (df_part_2_i_b_count_in_3['cumcount']+1)\n",
    "df_part_2_i_b_count_in_3['i_b2_count_in_3'] = df_part_2_i_b_count_in_3['behavior_type_2'] * (df_part_2_i_b_count_in_3['cumcount']+1)\n",
    "df_part_2_i_b_count_in_3['i_b3_count_in_3'] = df_part_2_i_b_count_in_3['behavior_type_3'] * (df_part_2_i_b_count_in_3['cumcount']+1)\n",
    "df_part_2_i_b_count_in_3['i_b4_count_in_3'] = df_part_2_i_b_count_in_3['behavior_type_4'] * (df_part_2_i_b_count_in_3['cumcount']+1)\n",
    "df_part_2_i_b_count_in_3 = df_part_2_i_b_count_in_3[['item_id', \n",
    "                                                     'i_b1_count_in_3', \n",
    "                                                     'i_b2_count_in_3', \n",
    "                                                     'i_b3_count_in_3',\n",
    "                                                     'i_b4_count_in_3']]\n",
    "df_part_2_i_b_count_in_3 = df_part_2_i_b_count_in_3.groupby('item_id').agg({'i_b1_count_in_3': np.sum,\n",
    "                                                                            'i_b2_count_in_3': np.sum,\n",
    "                                                                            'i_b3_count_in_3': np.sum,\n",
    "                                                                            'i_b4_count_in_3': np.sum})\n",
    "df_part_2_i_b_count_in_3.reset_index(inplace = True)\n",
    "df_part_2_i_b_count_in_3['i_b_count_in_3'] = df_part_2_i_b_count_in_3['i_b1_count_in_3'] + \\\n",
    "                                             df_part_2_i_b_count_in_3['i_b2_count_in_3'] + \\\n",
    "                                             df_part_2_i_b_count_in_3['i_b3_count_in_3'] + \\\n",
    "                                             df_part_2_i_b_count_in_3['i_b4_count_in_3']\n",
    "\n",
    "# i_b_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')]\n",
    "df_part_2_in_1['cumcount'] = df_part_2_in_1.groupby(['item_id', 'behavior_type']).cumcount()\n",
    "df_part_2_i_b_count_in_1 = df_part_2_in_1.drop_duplicates(['item_id','behavior_type'], 'last')[['item_id','behavior_type','cumcount']]\n",
    "df_part_2_i_b_count_in_1 = pd.get_dummies(df_part_2_i_b_count_in_1['behavior_type']).join(df_part_2_i_b_count_in_1[['item_id','cumcount']])\n",
    "df_part_2_i_b_count_in_1.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)                     \n",
    "df_part_2_i_b_count_in_1['i_b1_count_in_1'] = df_part_2_i_b_count_in_1['behavior_type_1'] * (df_part_2_i_b_count_in_1['cumcount']+1)\n",
    "df_part_2_i_b_count_in_1['i_b2_count_in_1'] = df_part_2_i_b_count_in_1['behavior_type_2'] * (df_part_2_i_b_count_in_1['cumcount']+1)\n",
    "df_part_2_i_b_count_in_1['i_b3_count_in_1'] = df_part_2_i_b_count_in_1['behavior_type_3'] * (df_part_2_i_b_count_in_1['cumcount']+1)\n",
    "df_part_2_i_b_count_in_1['i_b4_count_in_1'] = df_part_2_i_b_count_in_1['behavior_type_4'] * (df_part_2_i_b_count_in_1['cumcount']+1)\n",
    "df_part_2_i_b_count_in_1 = df_part_2_i_b_count_in_1[['item_id', \n",
    "                                                     'i_b1_count_in_1', \n",
    "                                                     'i_b2_count_in_1', \n",
    "                                                     'i_b3_count_in_1',\n",
    "                                                     'i_b4_count_in_1']]\n",
    "df_part_2_i_b_count_in_1 = df_part_2_i_b_count_in_1.groupby('item_id').agg({'i_b1_count_in_1': np.sum,\n",
    "                                                                            'i_b2_count_in_1': np.sum,\n",
    "                                                                            'i_b3_count_in_1': np.sum,\n",
    "                                                                            'i_b4_count_in_1': np.sum})\n",
    "df_part_2_i_b_count_in_1.reset_index(inplace = True)\n",
    "df_part_2_i_b_count_in_1['i_b_count_in_1'] = df_part_2_i_b_count_in_1['i_b1_count_in_1'] + \\\n",
    "                                             df_part_2_i_b_count_in_1['i_b2_count_in_1'] + \\\n",
    "                                             df_part_2_i_b_count_in_1['i_b3_count_in_1'] + \\\n",
    "                                             df_part_2_i_b_count_in_1['i_b4_count_in_1']\n",
    "\n",
    "# merge for generation of i_b_count\n",
    "df_part_2_i_b_count = pd.merge(df_part_2_i_b_count_in_6, \n",
    "                               df_part_2_i_b_count_in_3, \n",
    "                               on = ['item_id'], how = 'left').fillna(0)\n",
    "df_part_2_i_b_count = pd.merge(df_part_2_i_b_count, \n",
    "                               df_part_2_i_b_count_in_1, \n",
    "                               on = ['item_id'], how = 'left').fillna(0)\n",
    "df_part_2_i_b_count[['i_b1_count_in_6',\n",
    "                     'i_b2_count_in_6',\n",
    "                     'i_b3_count_in_6',\n",
    "                     'i_b4_count_in_6',\n",
    "                      'i_b_count_in_6',\n",
    "                     'i_b1_count_in_3',\n",
    "                     'i_b2_count_in_3',\n",
    "                     'i_b3_count_in_3',\n",
    "                     'i_b4_count_in_3',\n",
    "                      'i_b_count_in_3',\n",
    "                     'i_b1_count_in_1',\n",
    "                     'i_b2_count_in_1',\n",
    "                     'i_b3_count_in_1',\n",
    "                     'i_b4_count_in_1',\n",
    "                      'i_b_count_in_1']] = df_part_2_i_b_count[['i_b1_count_in_6',\n",
    "                                                                'i_b2_count_in_6',\n",
    "                                                                'i_b3_count_in_6',\n",
    "                                                                'i_b4_count_in_6',\n",
    "                                                                 'i_b_count_in_6',\n",
    "                                                                'i_b1_count_in_3',\n",
    "                                                                'i_b2_count_in_3',\n",
    "                                                                'i_b3_count_in_3',\n",
    "                                                                'i_b4_count_in_3',\n",
    "                                                                 'i_b_count_in_3',\n",
    "                                                                'i_b1_count_in_1',\n",
    "                                                                'i_b2_count_in_1',\n",
    "                                                                'i_b3_count_in_1',\n",
    "                                                                'i_b4_count_in_1',\n",
    "                                                                 'i_b_count_in_1']].astype(int)\n",
    "\n",
    "# i_b4_rate\n",
    "df_part_2_i_b_count['i_b4_rate'] = df_part_2_i_b_count['i_b4_count_in_6'] / df_part_2_i_b_count['i_b_count_in_6']\n",
    "\n",
    "# i_b4_diff_time\n",
    "df_part_2 = df_part_2.sort_values(by=['item_id', 'time'])\n",
    "df_part_2_i_b4_time = df_part_2[df_part_2['behavior_type'] == 4].drop_duplicates(['item_id'], 'first')[['item_id','time']]\n",
    "df_part_2_i_b4_time.columns = ['item_id','b4_first_time']\n",
    "df_part_2_i_b_time = df_part_2.drop_duplicates(['item_id'], 'first')[['item_id','time']]\n",
    "df_part_2_i_b_time.columns = ['item_id','b_first_time']\n",
    "df_part_2_i_b_b4_time = pd.merge(df_part_2_i_b_time, df_part_2_i_b4_time, on = ['item_id'])\n",
    "df_part_2_i_b_b4_time['i_b4_diff_time']  = df_part_2_i_b_b4_time['b4_first_time'] - df_part_2_i_b_b4_time['b_first_time']\n",
    "df_part_2_i_b_b4_time['i_b4_diff_hours'] = df_part_2_i_b_b4_time['i_b4_diff_time'].apply(lambda x: x.days * 24 + x.seconds//3600)\n",
    "df_part_2_i_b_b4_time = df_part_2_i_b_b4_time[['item_id', 'i_b4_diff_hours']]\n",
    "\n",
    "# generating feature set I\n",
    "f_I_part_2 = pd.merge(df_part_2_i_b_count, \n",
    "                      df_part_2_i_b_b4_time, \n",
    "                      on = ['item_id'], how = 'left')\n",
    "f_I_part_2 = pd.merge(f_I_part_2, \n",
    "                      df_part_2_i_u_count, \n",
    "                      on = ['item_id'], how = 'left')[['item_id', \n",
    "                                                       'i_u_count_in_6', \n",
    "                                                       'i_u_count_in_3', \n",
    "                                                       'i_u_count_in_1',\n",
    "                                                       'i_b1_count_in_6', \n",
    "                                                       'i_b2_count_in_6', \n",
    "                                                       'i_b3_count_in_6', \n",
    "                                                       'i_b4_count_in_6', \n",
    "                                                       'i_b_count_in_6', \n",
    "                                                       'i_b1_count_in_3',\n",
    "                                                       'i_b2_count_in_3',\n",
    "                                                       'i_b3_count_in_3',\n",
    "                                                       'i_b4_count_in_3',\n",
    "                                                       'i_b_count_in_3',\n",
    "                                                       'i_b1_count_in_1', \n",
    "                                                       'i_b2_count_in_1', \n",
    "                                                       'i_b3_count_in_1', \n",
    "                                                       'i_b4_count_in_1', \n",
    "                                                       'i_b_count_in_1',\n",
    "                                                       'i_b4_rate', \n",
    "                                                       'i_b4_diff_hours']]\n",
    "                      \n",
    "# write to csv file\n",
    "f_I_part_2 = f_I_part_2.round({'i_b4_rate': 3})\n",
    "f_I_part_2.to_csv(path_df_part_2_I, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "path_df = open(path_df_part_2, 'r')\n",
    "try:\n",
    "    df_part_2 = pd.read_csv(path_df, index_col = False, parse_dates = [0])\n",
    "    df_part_2.columns = ['time','user_id','item_id','behavior_type','item_category']\n",
    "finally:\n",
    "    path_df.close()\n",
    "    \n",
    "# c_u_count_in_6\n",
    "df_part_2_in_6 = df_part_2.drop_duplicates(['item_category', 'user_id'])\n",
    "df_part_2_in_6['c_u_count_in_6'] = df_part_2_in_6.groupby('item_category').cumcount() + 1\n",
    "df_part_2_c_u_count_in_6 = df_part_2_in_6.drop_duplicates(['item_category'], 'last')[['item_category', 'c_u_count_in_6']]\n",
    "\n",
    "# c_u_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')].drop_duplicates(['item_category', 'user_id'])\n",
    "df_part_2_in_3['c_u_count_in_3'] = df_part_2_in_3.groupby('item_category').cumcount() + 1\n",
    "df_part_2_c_u_count_in_3 = df_part_2_in_3.drop_duplicates(['item_category'], 'last')[['item_category', 'c_u_count_in_3']]\n",
    "\n",
    "# c_u_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')].drop_duplicates(['item_category', 'user_id'])\n",
    "df_part_2_in_1['c_u_count_in_1'] = df_part_2_in_1.groupby('item_category').cumcount() + 1\n",
    "df_part_2_c_u_count_in_1 = df_part_2_in_1.drop_duplicates(['item_category'], 'last')[['item_category', 'c_u_count_in_1']]\n",
    "\n",
    "df_part_2_c_u_count = pd.merge(df_part_2_c_u_count_in_6, df_part_2_c_u_count_in_3,on=['item_category'],how='left').fillna(0)\n",
    "df_part_2_c_u_count = pd.merge(df_part_2_c_u_count, df_part_2_c_u_count_in_1,on=['item_category'],how='left').fillna(0)\n",
    "df_part_2_c_u_count[['c_u_count_in_6',\n",
    "                     'c_u_count_in_3',\n",
    "                     'c_u_count_in_1']] = df_part_2_c_u_count[['c_u_count_in_6',\n",
    "                                                               'c_u_count_in_3',\n",
    "                                                               'c_u_count_in_1']].astype(int)\n",
    "\n",
    "# c_b_count_in_6\n",
    "df_part_2['cumcount'] = df_part_2.groupby(['item_category', 'behavior_type']).cumcount()\n",
    "df_part_2_c_b_count_in_6 = df_part_2.drop_duplicates(['item_category','behavior_type'], 'last')[['item_category','behavior_type','cumcount']]\n",
    "df_part_2_c_b_count_in_6 = pd.get_dummies(df_part_2_c_b_count_in_6['behavior_type']).join(df_part_2_c_b_count_in_6[['item_category','cumcount']])\n",
    "df_part_2_c_b_count_in_6.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_c_b_count_in_6['c_b1_count_in_6'] = df_part_2_c_b_count_in_6['behavior_type_1'] * (df_part_2_c_b_count_in_6['cumcount']+1)\n",
    "df_part_2_c_b_count_in_6['c_b2_count_in_6'] = df_part_2_c_b_count_in_6['behavior_type_2'] * (df_part_2_c_b_count_in_6['cumcount']+1)\n",
    "df_part_2_c_b_count_in_6['c_b3_count_in_6'] = df_part_2_c_b_count_in_6['behavior_type_3'] * (df_part_2_c_b_count_in_6['cumcount']+1)\n",
    "df_part_2_c_b_count_in_6['c_b4_count_in_6'] = df_part_2_c_b_count_in_6['behavior_type_4'] * (df_part_2_c_b_count_in_6['cumcount']+1)\n",
    "df_part_2_c_b_count_in_6 = df_part_2_c_b_count_in_6[['item_category', \n",
    "                                                     'c_b1_count_in_6', \n",
    "                                                     'c_b2_count_in_6', \n",
    "                                                     'c_b3_count_in_6',\n",
    "                                                     'c_b4_count_in_6']]\n",
    "df_part_2_c_b_count_in_6 = df_part_2_c_b_count_in_6.groupby('item_category').agg({'c_b1_count_in_6': np.sum,\n",
    "                                                                                  'c_b2_count_in_6': np.sum,\n",
    "                                                                                  'c_b3_count_in_6': np.sum,\n",
    "                                                                                  'c_b4_count_in_6': np.sum})\n",
    "df_part_2_c_b_count_in_6.reset_index(inplace = True)\n",
    "df_part_2_c_b_count_in_6['c_b_count_in_6'] = df_part_2_c_b_count_in_6['c_b1_count_in_6'] + \\\n",
    "                                             df_part_2_c_b_count_in_6['c_b2_count_in_6'] + \\\n",
    "                                             df_part_2_c_b_count_in_6['c_b3_count_in_6'] + \\\n",
    "                                             df_part_2_c_b_count_in_6['c_b4_count_in_6']\n",
    "\n",
    "# c_b_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')]\n",
    "df_part_2_in_3['cumcount'] = df_part_2_in_3.groupby(['item_category', 'behavior_type']).cumcount()\n",
    "df_part_2_c_b_count_in_3 = df_part_2_in_3.drop_duplicates(['item_category','behavior_type'], 'last')[['item_category','behavior_type','cumcount']]\n",
    "df_part_2_c_b_count_in_3 = pd.get_dummies(df_part_2_c_b_count_in_3['behavior_type']).join(df_part_2_c_b_count_in_3[['item_category','cumcount']])\n",
    "df_part_2_c_b_count_in_3.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_c_b_count_in_3['c_b1_count_in_3'] = df_part_2_c_b_count_in_3['behavior_type_1'] * (df_part_2_c_b_count_in_3['cumcount']+1)\n",
    "df_part_2_c_b_count_in_3['c_b2_count_in_3'] = df_part_2_c_b_count_in_3['behavior_type_2'] * (df_part_2_c_b_count_in_3['cumcount']+1)\n",
    "df_part_2_c_b_count_in_3['c_b3_count_in_3'] = df_part_2_c_b_count_in_3['behavior_type_3'] * (df_part_2_c_b_count_in_3['cumcount']+1)\n",
    "df_part_2_c_b_count_in_3['c_b4_count_in_3'] = df_part_2_c_b_count_in_3['behavior_type_4'] * (df_part_2_c_b_count_in_3['cumcount']+1)\n",
    "df_part_2_c_b_count_in_3 = df_part_2_c_b_count_in_3[['item_category', \n",
    "                                                     'c_b1_count_in_3', \n",
    "                                                     'c_b2_count_in_3', \n",
    "                                                     'c_b3_count_in_3',\n",
    "                                                     'c_b4_count_in_3']]\n",
    "df_part_2_c_b_count_in_3 = df_part_2_c_b_count_in_3.groupby('item_category').agg({'c_b1_count_in_3': np.sum,\n",
    "                                                                                  'c_b2_count_in_3': np.sum,\n",
    "                                                                                  'c_b3_count_in_3': np.sum,\n",
    "                                                                                  'c_b4_count_in_3': np.sum})\n",
    "df_part_2_c_b_count_in_3.reset_index(inplace = True)\n",
    "df_part_2_c_b_count_in_3['c_b_count_in_3'] = df_part_2_c_b_count_in_3['c_b1_count_in_3'] + \\\n",
    "                                             df_part_2_c_b_count_in_3['c_b2_count_in_3'] + \\\n",
    "                                             df_part_2_c_b_count_in_3['c_b3_count_in_3'] + \\\n",
    "                                             df_part_2_c_b_count_in_3['c_b4_count_in_3']\n",
    "\n",
    "# c_b_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')]\n",
    "df_part_2_in_1['cumcount'] = df_part_2_in_1.groupby(['item_category', 'behavior_type']).cumcount()\n",
    "df_part_2_c_b_count_in_1 = df_part_2_in_1.drop_duplicates(['item_category','behavior_type'], 'last')[['item_category','behavior_type','cumcount']]\n",
    "df_part_2_c_b_count_in_1 = pd.get_dummies(df_part_2_c_b_count_in_1['behavior_type']).join(df_part_2_c_b_count_in_1[['item_category','cumcount']])\n",
    "df_part_2_c_b_count_in_1.rename(columns = {1:'behavior_type_1',\n",
    "                                           2:'behavior_type_2',\n",
    "                                           3:'behavior_type_3',\n",
    "                                           4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_c_b_count_in_1['c_b1_count_in_1'] = df_part_2_c_b_count_in_1['behavior_type_1'] * (df_part_2_c_b_count_in_1['cumcount']+1)\n",
    "df_part_2_c_b_count_in_1['c_b2_count_in_1'] = df_part_2_c_b_count_in_1['behavior_type_2'] * (df_part_2_c_b_count_in_1['cumcount']+1)\n",
    "df_part_2_c_b_count_in_1['c_b3_count_in_1'] = df_part_2_c_b_count_in_1['behavior_type_3'] * (df_part_2_c_b_count_in_1['cumcount']+1)\n",
    "df_part_2_c_b_count_in_1['c_b4_count_in_1'] = df_part_2_c_b_count_in_1['behavior_type_4'] * (df_part_2_c_b_count_in_1['cumcount']+1)\n",
    "df_part_2_c_b_count_in_1 = df_part_2_c_b_count_in_1[['item_category', \n",
    "                                                     'c_b1_count_in_1', \n",
    "                                                     'c_b2_count_in_1', \n",
    "                                                     'c_b3_count_in_1',\n",
    "                                                     'c_b4_count_in_1']]\n",
    "df_part_2_c_b_count_in_1 = df_part_2_c_b_count_in_1.groupby('item_category').agg({'c_b1_count_in_1': np.sum,\n",
    "                                                                                  'c_b2_count_in_1': np.sum,\n",
    "                                                                                  'c_b3_count_in_1': np.sum,\n",
    "                                                                                  'c_b4_count_in_1': np.sum})\n",
    "df_part_2_c_b_count_in_1.reset_index(inplace = True)\n",
    "df_part_2_c_b_count_in_1['c_b_count_in_1'] = df_part_2_c_b_count_in_1['c_b1_count_in_1'] + \\\n",
    "                                             df_part_2_c_b_count_in_1['c_b2_count_in_1'] + \\\n",
    "                                             df_part_2_c_b_count_in_1['c_b3_count_in_1'] + \\\n",
    "                                             df_part_2_c_b_count_in_1['c_b4_count_in_1']    \n",
    "                                             \n",
    "df_part_2_c_b_count = pd.merge(df_part_2_c_b_count_in_6, df_part_2_c_b_count_in_3, on = ['item_category'], how = 'left').fillna(0)                                      \n",
    "df_part_2_c_b_count = pd.merge(df_part_2_c_b_count, df_part_2_c_b_count_in_1, on = ['item_category'], how = 'left').fillna(0)\n",
    "df_part_2_c_b_count[['c_b1_count_in_6',\n",
    "                     'c_b2_count_in_6',\n",
    "                     'c_b3_count_in_6',\n",
    "                     'c_b4_count_in_6',\n",
    "                      'c_b_count_in_6',\n",
    "                     'c_b1_count_in_3',\n",
    "                     'c_b2_count_in_3',\n",
    "                     'c_b3_count_in_3',\n",
    "                     'c_b4_count_in_3',\n",
    "                      'c_b_count_in_3',\n",
    "                     'c_b1_count_in_1',\n",
    "                     'c_b2_count_in_1',\n",
    "                     'c_b3_count_in_1',\n",
    "                     'c_b4_count_in_1',\n",
    "                      'c_b_count_in_1']] = df_part_2_c_b_count[['c_b1_count_in_6',\n",
    "                                                                'c_b2_count_in_6',\n",
    "                                                                'c_b3_count_in_6',\n",
    "                                                                'c_b4_count_in_6',\n",
    "                                                                 'c_b_count_in_6',\n",
    "                                                                'c_b1_count_in_3',\n",
    "                                                                'c_b2_count_in_3',\n",
    "                                                                'c_b3_count_in_3',\n",
    "                                                                'c_b4_count_in_3',\n",
    "                                                                 'c_b_count_in_3',\n",
    "                                                                'c_b1_count_in_1',\n",
    "                                                                'c_b2_count_in_1',\n",
    "                                                                'c_b3_count_in_1',\n",
    "                                                                'c_b4_count_in_1',\n",
    "                                                                 'c_b_count_in_1']].astype(int)\n",
    "\n",
    "# c_b4_rate\n",
    "df_part_2_c_b_count['c_b4_rate'] = df_part_2_c_b_count['c_b4_count_in_6'] / df_part_2_c_b_count['c_b_count_in_6']\n",
    "\n",
    "# c_b4_diff_time\n",
    "df_part_2 = df_part_2.sort_values(by=['item_category', 'time'])\n",
    "df_part_2_c_b4_time = df_part_2[df_part_2['behavior_type'] == 4].drop_duplicates(['item_category'], 'first')[['item_category','time']]\n",
    "df_part_2_c_b4_time.columns = ['item_category','b4_first_time']\n",
    "df_part_2_c_b_time = df_part_2.drop_duplicates(['item_category'], 'first')[['item_category','time']]\n",
    "df_part_2_c_b_time.columns = ['item_category','b_first_time']\n",
    "df_part_2_c_b_b4_time = pd.merge(df_part_2_c_b_time, df_part_2_c_b4_time, on = ['item_category'])\n",
    "df_part_2_c_b_b4_time['c_b4_diff_time']  = df_part_2_c_b_b4_time['b4_first_time'] - df_part_2_c_b_b4_time['b_first_time']\n",
    "df_part_2_c_b_b4_time['c_b4_diff_hours'] = df_part_2_c_b_b4_time['c_b4_diff_time'].apply(lambda x: x.days * 24 + x.seconds//3600)\n",
    "df_part_2_c_b_b4_time = df_part_2_c_b_b4_time[['item_category',\n",
    "                                               'c_b4_diff_hours']]\n",
    "\n",
    "# generating feature set C\n",
    "f_C_part_2 = pd.merge(df_part_2_c_u_count, df_part_2_c_b_count, on = ['item_category'], how = 'left')\n",
    "f_C_part_2 = pd.merge(f_C_part_2, df_part_2_c_b_b4_time, on = ['item_category'], how = 'left')\n",
    "f_C_part_2 = f_C_part_2.round({'c_b4_rate': 3})\n",
    "\n",
    "# write to csv file\n",
    "f_C_part_2.to_csv(path_df_part_2_C, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df_part_2_i_ub_count\n",
    "path_df = open(path_df_part_2_I, 'r')\n",
    "try:\n",
    "    df_part_2_I = pd.read_csv(path_df, index_col = False)\n",
    "finally:\n",
    "    path_df.close()\n",
    "df_part_2_i_ub_count = df_part_2_I[['item_id','i_u_count_in_6','i_b_count_in_6','i_b4_count_in_6']]\n",
    "del(df_part_2_I)\n",
    "\n",
    "# get df_part_2_uic for merge i & c\n",
    "path_df = open(path_df_part_2_uic_label, 'r')\n",
    "try:\n",
    "    df_part_2_uic = pd.read_csv(path_df, index_col = False)\n",
    "finally:\n",
    "    path_df.close()\n",
    "df_part_2_ic_u_b_count = pd.merge(df_part_2_uic, df_part_2_i_ub_count, on=['item_id'], how='left').fillna(0)\n",
    "df_part_2_ic_u_b_count = df_part_2_ic_u_b_count.drop_duplicates(['item_id','item_category'])\n",
    "\n",
    "# ic_u_rank_in_c\n",
    "df_part_2_ic_u_b_count['ic_u_rank_in_c'] = df_part_2_ic_u_b_count.groupby('item_category')['i_u_count_in_6'].rank(method='min',ascending=False).astype('int')\n",
    "# ic_b_rank_in_c\n",
    "df_part_2_ic_u_b_count['ic_b_rank_in_c'] = df_part_2_ic_u_b_count.groupby('item_category')['i_b_count_in_6'].rank(method='min',ascending=False).astype('int')\n",
    "# ic_b4_rank_in_c\n",
    "df_part_2_ic_u_b_count['ic_b4_rank_in_c'] = df_part_2_ic_u_b_count.groupby('item_category')['i_b4_count_in_6'].rank(method='min',ascending=False).astype('int')\n",
    "\n",
    "f_IC_part_2 = df_part_2_ic_u_b_count[['item_id', \n",
    "                                      'item_category', \n",
    "                                      'ic_u_rank_in_c', \n",
    "                                      'ic_b_rank_in_c', \n",
    "                                      'ic_b4_rank_in_c']]\n",
    "# write to csv file\n",
    "f_IC_part_2.to_csv(path_df_part_2_IC, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "path_df = open(path_df_part_2, 'r')\n",
    "try:\n",
    "    df_part_2 = pd.read_csv(path_df, index_col = False, parse_dates = [0])\n",
    "    df_part_2.columns = ['time','user_id','item_id','behavior_type','item_category']\n",
    "finally:\n",
    "    path_df.close()\n",
    "\n",
    "# ui_b_count_in_6\n",
    "df_part_2['cumcount'] = df_part_2.groupby(['user_id', 'item_id', 'behavior_type']).cumcount()\n",
    "df_part_2_ui_b_count_in_6 = df_part_2.drop_duplicates(['user_id','item_id','behavior_type'],'last')[['user_id','item_id','behavior_type','cumcount']]\n",
    "df_part_2_ui_b_count_in_6 = pd.get_dummies(df_part_2_ui_b_count_in_6['behavior_type']).join(df_part_2_ui_b_count_in_6[['user_id','item_id','cumcount']])\n",
    "df_part_2_ui_b_count_in_6.rename(columns = {1:'behavior_type_1',\n",
    "                                            2:'behavior_type_2',\n",
    "                                            3:'behavior_type_3',\n",
    "                                            4:'behavior_type_4'}, inplace=True)  \n",
    "df_part_2_ui_b_count_in_6['ui_b1_count_in_6'] = df_part_2_ui_b_count_in_6['behavior_type_1'] * (df_part_2_ui_b_count_in_6['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_6['ui_b2_count_in_6'] = df_part_2_ui_b_count_in_6['behavior_type_2'] * (df_part_2_ui_b_count_in_6['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_6['ui_b3_count_in_6'] = df_part_2_ui_b_count_in_6['behavior_type_3'] * (df_part_2_ui_b_count_in_6['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_6['ui_b4_count_in_6'] = df_part_2_ui_b_count_in_6['behavior_type_4'] * (df_part_2_ui_b_count_in_6['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_6 = df_part_2_ui_b_count_in_6[['user_id', \n",
    "                                                       'item_id', \n",
    "                                                       'ui_b1_count_in_6', \n",
    "                                                       'ui_b2_count_in_6', \n",
    "                                                       'ui_b3_count_in_6',\n",
    "                                                       'ui_b4_count_in_6']]\n",
    "df_part_2_ui_b_count_in_6 = df_part_2_ui_b_count_in_6.groupby(['user_id', 'item_id']).agg({'ui_b1_count_in_6': np.sum,\n",
    "                                                                                           'ui_b2_count_in_6': np.sum,\n",
    "                                                                                           'ui_b3_count_in_6': np.sum,\n",
    "                                                                                           'ui_b4_count_in_6': np.sum})\n",
    "df_part_2_ui_b_count_in_6.reset_index(inplace = True)\n",
    "df_part_2_ui_b_count_in_6['ui_b_count_in_6'] = df_part_2_ui_b_count_in_6['ui_b1_count_in_6'] + \\\n",
    "                                               df_part_2_ui_b_count_in_6['ui_b2_count_in_6'] + \\\n",
    "                                               df_part_2_ui_b_count_in_6['ui_b3_count_in_6'] + \\\n",
    "                                               df_part_2_ui_b_count_in_6['ui_b4_count_in_6']\n",
    "\n",
    "# ui_b_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')]\n",
    "df_part_2_in_3['cumcount'] = df_part_2_in_3.groupby(['user_id', 'item_id', 'behavior_type']).cumcount()\n",
    "df_part_2_ui_b_count_in_3 = df_part_2.drop_duplicates(['user_id','item_id','behavior_type'],'last')[['user_id','item_id','behavior_type','cumcount']]\n",
    "df_part_2_ui_b_count_in_3 = pd.get_dummies(df_part_2_ui_b_count_in_3['behavior_type']).join(df_part_2_ui_b_count_in_3[['user_id','item_id','cumcount']])\n",
    "df_part_2_ui_b_count_in_3.rename(columns = {1:'behavior_type_1',\n",
    "                                            2:'behavior_type_2',\n",
    "                                            3:'behavior_type_3',\n",
    "                                            4:'behavior_type_4'}, inplace=True)  \n",
    "df_part_2_ui_b_count_in_3['ui_b1_count_in_3'] = df_part_2_ui_b_count_in_3['behavior_type_1'] * (df_part_2_ui_b_count_in_3['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_3['ui_b2_count_in_3'] = df_part_2_ui_b_count_in_3['behavior_type_2'] * (df_part_2_ui_b_count_in_3['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_3['ui_b3_count_in_3'] = df_part_2_ui_b_count_in_3['behavior_type_3'] * (df_part_2_ui_b_count_in_3['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_3['ui_b4_count_in_3'] = df_part_2_ui_b_count_in_3['behavior_type_4'] * (df_part_2_ui_b_count_in_3['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_3 = df_part_2_ui_b_count_in_3[['user_id', \n",
    "                                                       'item_id', \n",
    "                                                       'ui_b1_count_in_3', \n",
    "                                                       'ui_b2_count_in_3', \n",
    "                                                       'ui_b3_count_in_3',\n",
    "                                                       'ui_b4_count_in_3']]\n",
    "df_part_2_ui_b_count_in_3 = df_part_2_ui_b_count_in_3.groupby(['user_id', 'item_id']).agg({'ui_b1_count_in_3': np.sum,\n",
    "                                                                                           'ui_b2_count_in_3': np.sum,\n",
    "                                                                                           'ui_b3_count_in_3': np.sum,\n",
    "                                                                                           'ui_b4_count_in_3': np.sum})\n",
    "df_part_2_ui_b_count_in_3.reset_index(inplace = True)\n",
    "df_part_2_ui_b_count_in_3['ui_b_count_in_3'] = df_part_2_ui_b_count_in_3['ui_b1_count_in_3'] + \\\n",
    "                                               df_part_2_ui_b_count_in_3['ui_b2_count_in_3'] + \\\n",
    "                                               df_part_2_ui_b_count_in_3['ui_b3_count_in_3'] + \\\n",
    "                                               df_part_2_ui_b_count_in_3['ui_b4_count_in_3']\n",
    "\n",
    "# ui_b_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')]\n",
    "df_part_2_in_1['cumcount'] = df_part_2_in_1.groupby(['user_id', 'item_id', 'behavior_type']).cumcount()\n",
    "df_part_2_ui_b_count_in_1 = df_part_2_in_1.drop_duplicates(['user_id','item_id','behavior_type'], 'last')[['user_id','item_id','behavior_type','cumcount']]\n",
    "df_part_2_ui_b_count_in_1 = pd.get_dummies(df_part_2_ui_b_count_in_1['behavior_type']).join(df_part_2_ui_b_count_in_1[['user_id','item_id','cumcount']])\n",
    "df_part_2_ui_b_count_in_1.rename(columns = {1:'behavior_type_1',\n",
    "                                            2:'behavior_type_2',\n",
    "                                            3:'behavior_type_3',\n",
    "                                            4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_ui_b_count_in_1['ui_b1_count_in_1'] = df_part_2_ui_b_count_in_1['behavior_type_1'] * (df_part_2_ui_b_count_in_1['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_1['ui_b2_count_in_1'] = df_part_2_ui_b_count_in_1['behavior_type_2'] * (df_part_2_ui_b_count_in_1['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_1['ui_b3_count_in_1'] = df_part_2_ui_b_count_in_1['behavior_type_3'] * (df_part_2_ui_b_count_in_1['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_1['ui_b4_count_in_1'] = df_part_2_ui_b_count_in_1['behavior_type_4'] * (df_part_2_ui_b_count_in_1['cumcount']+1)\n",
    "df_part_2_ui_b_count_in_1 = df_part_2_ui_b_count_in_1[['user_id',\n",
    "                                                       'item_id', \n",
    "                                                       'ui_b1_count_in_1', \n",
    "                                                       'ui_b2_count_in_1', \n",
    "                                                       'ui_b3_count_in_1',\n",
    "                                                       'ui_b4_count_in_1']]\n",
    "df_part_2_ui_b_count_in_1 = df_part_2_ui_b_count_in_1.groupby(['user_id', 'item_id']).agg({'ui_b1_count_in_1': np.sum,\n",
    "                                                                                           'ui_b2_count_in_1': np.sum,\n",
    "                                                                                           'ui_b3_count_in_1': np.sum,\n",
    "                                                                                           'ui_b4_count_in_1': np.sum})\n",
    "df_part_2_ui_b_count_in_1.reset_index(inplace = True)\n",
    "df_part_2_ui_b_count_in_1['ui_b_count_in_1'] = df_part_2_ui_b_count_in_1['ui_b1_count_in_1'] + \\\n",
    "                                               df_part_2_ui_b_count_in_1['ui_b2_count_in_1'] + \\\n",
    "                                               df_part_2_ui_b_count_in_1['ui_b3_count_in_1'] + \\\n",
    "                                               df_part_2_ui_b_count_in_1['ui_b4_count_in_1']\n",
    "                                             \n",
    "df_part_2_ui_b_count = pd.merge(df_part_2_ui_b_count_in_6, df_part_2_ui_b_count_in_3, on = ['user_id','item_id'], how = 'left').fillna(0)\n",
    "df_part_2_ui_b_count = pd.merge(df_part_2_ui_b_count, df_part_2_ui_b_count_in_1, on = ['user_id','item_id'], how = 'left').fillna(0)\n",
    "df_part_2_ui_b_count[['ui_b1_count_in_6',\n",
    "                      'ui_b2_count_in_6',\n",
    "                      'ui_b3_count_in_6',\n",
    "                      'ui_b4_count_in_6',\n",
    "                       'ui_b_count_in_6',\n",
    "                      'ui_b1_count_in_3',\n",
    "                      'ui_b2_count_in_3',\n",
    "                      'ui_b3_count_in_3',\n",
    "                      'ui_b4_count_in_3',\n",
    "                       'ui_b_count_in_3',\n",
    "                      'ui_b1_count_in_1',\n",
    "                      'ui_b2_count_in_1',\n",
    "                      'ui_b3_count_in_1',\n",
    "                      'ui_b4_count_in_1',\n",
    "                       'ui_b_count_in_1']] = df_part_2_ui_b_count[['ui_b1_count_in_6',\n",
    "                                                                   'ui_b2_count_in_6',\n",
    "                                                                   'ui_b3_count_in_6',\n",
    "                                                                   'ui_b4_count_in_6',\n",
    "                                                                    'ui_b_count_in_6',\n",
    "                                                                   'ui_b1_count_in_3',\n",
    "                                                                   'ui_b2_count_in_3',\n",
    "                                                                   'ui_b3_count_in_3',\n",
    "                                                                   'ui_b4_count_in_3',\n",
    "                                                                    'ui_b_count_in_3',\n",
    "                                                                   'ui_b1_count_in_1',\n",
    "                                                                   'ui_b2_count_in_1',\n",
    "                                                                   'ui_b3_count_in_1',\n",
    "                                                                   'ui_b4_count_in_1',\n",
    "                                                                    'ui_b_count_in_1']].astype(int)\n",
    "\n",
    "# ui_b_count_rank_in_u\n",
    "df_part_2_ui_b_count['ui_b_count_rank_in_u'] = df_part_2_ui_b_count.groupby(['user_id'])['ui_b_count_in_6'].rank(method='min',ascending=False).astype('int')\n",
    "\n",
    "# ui_b_count_rank_in_uc\n",
    "path_df = open(path_df_part_2_uic_label, 'r')\n",
    "try:\n",
    "    df_part_2_uic = pd.read_csv(path_df, index_col = False)\n",
    "finally:\n",
    "    path_df.close()\n",
    "df_part_2_ui_b_count = pd.merge(df_part_2_uic, df_part_2_ui_b_count, on = ['user_id','item_id'], how = 'left')\n",
    "df_part_2_ui_b_count['ui_b_count_rank_in_uc'] = df_part_2_ui_b_count.groupby(['user_id','item_category'])['ui_b_count_rank_in_u'].rank(method='min',ascending=True).astype('int')\n",
    "\n",
    "\n",
    "# ui_b_last_time\n",
    "df_part_2.sort_values(by=['user_id','item_id','behavior_type','time'], inplace=True)\n",
    "df_part_2_ui_b_last_time = df_part_2.drop_duplicates(['user_id','item_id','behavior_type'],'last')[['user_id','item_id','behavior_type','time']]\n",
    "\n",
    "df_part_2_ui_b_last_time['ui_b1_last_time'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['behavior_type'] == 1]['time']\n",
    "df_part_2_ui_b_last_time['ui_b2_last_time'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['behavior_type'] == 2]['time']\n",
    "df_part_2_ui_b_last_time['ui_b3_last_time'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['behavior_type'] == 3]['time']\n",
    "df_part_2_ui_b_last_time['ui_b4_last_time'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['behavior_type'] == 4]['time']\n",
    "\n",
    "df_part_2_ui_b_last_time.loc[df_part_2_ui_b_last_time['ui_b1_last_time'].notnull(), 'ui_b1_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_ui_b_last_time['ui_b1_last_time'])             \n",
    "df_part_2_ui_b_last_time['ui_b1_last_hours'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['ui_b1_last_hours'].notnull()]['ui_b1_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_ui_b_last_time.loc[df_part_2_ui_b_last_time['ui_b2_last_time'].notnull(), 'ui_b2_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_ui_b_last_time['ui_b2_last_time'])             \n",
    "df_part_2_ui_b_last_time['ui_b2_last_hours'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['ui_b2_last_hours'].notnull()]['ui_b2_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_ui_b_last_time.loc[df_part_2_ui_b_last_time['ui_b3_last_time'].notnull(), 'ui_b3_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_ui_b_last_time['ui_b3_last_time'])             \n",
    "df_part_2_ui_b_last_time['ui_b3_last_hours'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['ui_b3_last_hours'].notnull()]['ui_b3_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_ui_b_last_time.loc[df_part_2_ui_b_last_time['ui_b4_last_time'].notnull(), 'ui_b4_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_ui_b_last_time['ui_b4_last_time'])             \n",
    "df_part_2_ui_b_last_time['ui_b4_last_hours'] = df_part_2_ui_b_last_time[df_part_2_ui_b_last_time['ui_b4_last_hours'].notnull()]['ui_b4_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_ui_b_last_time = df_part_2_ui_b_last_time[['user_id',\n",
    "                                                     'item_id',\n",
    "                                                     'ui_b1_last_hours',\n",
    "                                                     'ui_b2_last_hours',\n",
    "                                                     'ui_b3_last_hours',\n",
    "                                                     'ui_b4_last_hours']] \n",
    "\n",
    "df_part_2_ui_b_last_time = df_part_2_ui_b_last_time.groupby(['user_id', 'item_id']).agg({'ui_b1_last_hours': np.sum,\n",
    "                                                                                         'ui_b2_last_hours': np.sum,\n",
    "                                                                                         'ui_b3_last_hours': np.sum,\n",
    "                                                                                         'ui_b4_last_hours': np.sum})\n",
    "df_part_2_ui_b_last_time.reset_index(inplace = True)\n",
    "\n",
    "# merge for generation of f_UI_part_2\n",
    "f_UI_part_2 = pd.merge(df_part_2_ui_b_count, df_part_2_ui_b_last_time, how='left', on=['user_id', 'item_id'])\n",
    "\n",
    "# write to csv file\n",
    "f_UI_part_2.to_csv(path_df_part_2_UI, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "path_df = open(path_df_part_2, 'r')\n",
    "try:\n",
    "    df_part_2 = pd.read_csv(path_df, index_col = False, parse_dates = [0])\n",
    "    df_part_2.columns = ['time','user_id','item_id','behavior_type','item_category']\n",
    "finally:\n",
    "    path_df.close()\n",
    "\n",
    "# uc_b_count_in_6\n",
    "df_part_2['cumcount'] = df_part_2.groupby(['user_id', 'item_category', 'behavior_type']).cumcount()\n",
    "df_part_2_uc_b_count_in_6 = df_part_2.drop_duplicates(['user_id','item_category','behavior_type'],'last')[['user_id','item_category','behavior_type','cumcount']]\n",
    "df_part_2_uc_b_count_in_6 = pd.get_dummies(df_part_2_uc_b_count_in_6['behavior_type']).join(df_part_2_uc_b_count_in_6[['user_id','item_category','cumcount']])\n",
    "df_part_2_uc_b_count_in_6.rename(columns = {1:'behavior_type_1',\n",
    "                                            2:'behavior_type_2',\n",
    "                                            3:'behavior_type_3',\n",
    "                                            4:'behavior_type_4'}, inplace=True)  \n",
    "df_part_2_uc_b_count_in_6['uc_b1_count_in_6'] = df_part_2_uc_b_count_in_6['behavior_type_1'] * (df_part_2_uc_b_count_in_6['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_6['uc_b2_count_in_6'] = df_part_2_uc_b_count_in_6['behavior_type_2'] * (df_part_2_uc_b_count_in_6['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_6['uc_b3_count_in_6'] = df_part_2_uc_b_count_in_6['behavior_type_3'] * (df_part_2_uc_b_count_in_6['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_6['uc_b4_count_in_6'] = df_part_2_uc_b_count_in_6['behavior_type_4'] * (df_part_2_uc_b_count_in_6['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_6 = df_part_2_uc_b_count_in_6[['user_id', \n",
    "                                                       'item_category', \n",
    "                                                       'uc_b1_count_in_6', \n",
    "                                                       'uc_b2_count_in_6', \n",
    "                                                       'uc_b3_count_in_6',\n",
    "                                                       'uc_b4_count_in_6']]\n",
    "df_part_2_uc_b_count_in_6 = df_part_2_uc_b_count_in_6.groupby(['user_id', 'item_category']).agg({'uc_b1_count_in_6': np.sum,\n",
    "                                                                                                 'uc_b2_count_in_6': np.sum,\n",
    "                                                                                                 'uc_b3_count_in_6': np.sum,\n",
    "                                                                                                 'uc_b4_count_in_6': np.sum})\n",
    "df_part_2_uc_b_count_in_6.reset_index(inplace = True)\n",
    "df_part_2_uc_b_count_in_6['uc_b_count_in_6'] = df_part_2_uc_b_count_in_6['uc_b1_count_in_6'] + \\\n",
    "                                               df_part_2_uc_b_count_in_6['uc_b2_count_in_6'] + \\\n",
    "                                               df_part_2_uc_b_count_in_6['uc_b3_count_in_6'] + \\\n",
    "                                               df_part_2_uc_b_count_in_6['uc_b4_count_in_6']\n",
    "\n",
    "# uc_b_count_in_3\n",
    "df_part_2_in_3 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-02')]\n",
    "df_part_2_in_3['cumcount'] = df_part_2_in_3.groupby(['user_id', 'item_category', 'behavior_type']).cumcount()\n",
    "df_part_2_uc_b_count_in_3 = df_part_2.drop_duplicates(['user_id','item_category','behavior_type'],'last')[['user_id','item_category','behavior_type','cumcount']]\n",
    "df_part_2_uc_b_count_in_3 = pd.get_dummies(df_part_2_uc_b_count_in_3['behavior_type']).join(df_part_2_uc_b_count_in_3[['user_id','item_category','cumcount']])\n",
    "df_part_2_uc_b_count_in_3.rename(columns = {1:'behavior_type_1',\n",
    "                                            2:'behavior_type_2',\n",
    "                                            3:'behavior_type_3',\n",
    "                                            4:'behavior_type_4'}, inplace=True)  \n",
    "df_part_2_uc_b_count_in_3['uc_b1_count_in_3'] = df_part_2_uc_b_count_in_3['behavior_type_1'] * (df_part_2_uc_b_count_in_3['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_3['uc_b2_count_in_3'] = df_part_2_uc_b_count_in_3['behavior_type_2'] * (df_part_2_uc_b_count_in_3['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_3['uc_b3_count_in_3'] = df_part_2_uc_b_count_in_3['behavior_type_3'] * (df_part_2_uc_b_count_in_3['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_3['uc_b4_count_in_3'] = df_part_2_uc_b_count_in_3['behavior_type_4'] * (df_part_2_uc_b_count_in_3['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_3 = df_part_2_uc_b_count_in_3[['user_id', \n",
    "                                                       'item_category', \n",
    "                                                       'uc_b1_count_in_3', \n",
    "                                                       'uc_b2_count_in_3', \n",
    "                                                       'uc_b3_count_in_3',\n",
    "                                                       'uc_b4_count_in_3']]\n",
    "df_part_2_uc_b_count_in_3 = df_part_2_uc_b_count_in_3.groupby(['user_id', 'item_category']).agg({'uc_b1_count_in_3': np.sum,\n",
    "                                                                                                 'uc_b2_count_in_3': np.sum,\n",
    "                                                                                                 'uc_b3_count_in_3': np.sum,\n",
    "                                                                                                 'uc_b4_count_in_3': np.sum})\n",
    "df_part_2_uc_b_count_in_3.reset_index(inplace = True)\n",
    "df_part_2_uc_b_count_in_3['uc_b_count_in_3'] = df_part_2_uc_b_count_in_3['uc_b1_count_in_3'] + \\\n",
    "                                               df_part_2_uc_b_count_in_3['uc_b2_count_in_3'] + \\\n",
    "                                               df_part_2_uc_b_count_in_3['uc_b3_count_in_3'] + \\\n",
    "                                               df_part_2_uc_b_count_in_3['uc_b4_count_in_3']\n",
    "\n",
    "# uc_b_count_in_1\n",
    "df_part_2_in_1 = df_part_2[df_part_2['time'] >= np.datetime64('2014-12-04')]\n",
    "df_part_2_in_1['cumcount'] = df_part_2_in_1.groupby(['user_id', 'item_category', 'behavior_type']).cumcount()\n",
    "df_part_2_uc_b_count_in_1 = df_part_2_in_1.drop_duplicates(['user_id','item_category','behavior_type'], 'last')[['user_id','item_category','behavior_type','cumcount']]\n",
    "df_part_2_uc_b_count_in_1 = pd.get_dummies(df_part_2_uc_b_count_in_1['behavior_type']).join(df_part_2_uc_b_count_in_1[['user_id','item_category','cumcount']])\n",
    "df_part_2_uc_b_count_in_1.rename(columns = {1:'behavior_type_1',\n",
    "                                            2:'behavior_type_2',\n",
    "                                            3:'behavior_type_3',\n",
    "                                            4:'behavior_type_4'}, inplace=True)\n",
    "df_part_2_uc_b_count_in_1['uc_b1_count_in_1'] = df_part_2_uc_b_count_in_1['behavior_type_1'] * (df_part_2_uc_b_count_in_1['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_1['uc_b2_count_in_1'] = df_part_2_uc_b_count_in_1['behavior_type_2'] * (df_part_2_uc_b_count_in_1['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_1['uc_b3_count_in_1'] = df_part_2_uc_b_count_in_1['behavior_type_3'] * (df_part_2_uc_b_count_in_1['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_1['uc_b4_count_in_1'] = df_part_2_uc_b_count_in_1['behavior_type_4'] * (df_part_2_uc_b_count_in_1['cumcount']+1)\n",
    "df_part_2_uc_b_count_in_1 = df_part_2_uc_b_count_in_1[['user_id',\n",
    "                                                       'item_category', \n",
    "                                                       'uc_b1_count_in_1', \n",
    "                                                       'uc_b2_count_in_1', \n",
    "                                                       'uc_b3_count_in_1',\n",
    "                                                       'uc_b4_count_in_1']]\n",
    "df_part_2_uc_b_count_in_1 = df_part_2_uc_b_count_in_1.groupby(['user_id', 'item_category']).agg({'uc_b1_count_in_1': np.sum,\n",
    "                                                                                                 'uc_b2_count_in_1': np.sum,\n",
    "                                                                                                 'uc_b3_count_in_1': np.sum,\n",
    "                                                                                                 'uc_b4_count_in_1': np.sum})\n",
    "df_part_2_uc_b_count_in_1.reset_index(inplace = True)\n",
    "df_part_2_uc_b_count_in_1['uc_b_count_in_1'] = df_part_2_uc_b_count_in_1['uc_b1_count_in_1'] + \\\n",
    "                                               df_part_2_uc_b_count_in_1['uc_b2_count_in_1'] + \\\n",
    "                                               df_part_2_uc_b_count_in_1['uc_b3_count_in_1'] + \\\n",
    "                                               df_part_2_uc_b_count_in_1['uc_b4_count_in_1']\n",
    "                                             \n",
    "df_part_2_uc_b_count = pd.merge(df_part_2_uc_b_count_in_6, df_part_2_uc_b_count_in_3, on = ['user_id','item_category'], how = 'left').fillna(0)\n",
    "df_part_2_uc_b_count = pd.merge(df_part_2_uc_b_count, df_part_2_uc_b_count_in_1, on = ['user_id','item_category'], how = 'left').fillna(0)\n",
    "df_part_2_uc_b_count[['uc_b1_count_in_6',\n",
    "                      'uc_b2_count_in_6',\n",
    "                      'uc_b3_count_in_6',\n",
    "                      'uc_b4_count_in_6',\n",
    "                       'uc_b_count_in_6',\n",
    "                      'uc_b1_count_in_3',\n",
    "                      'uc_b2_count_in_3',\n",
    "                      'uc_b3_count_in_3',\n",
    "                      'uc_b4_count_in_3',\n",
    "                       'uc_b_count_in_3',\n",
    "                      'uc_b1_count_in_1',\n",
    "                      'uc_b2_count_in_1',\n",
    "                      'uc_b3_count_in_1',\n",
    "                      'uc_b4_count_in_1',\n",
    "                       'uc_b_count_in_1']] = df_part_2_uc_b_count[['uc_b1_count_in_6',\n",
    "                                                                   'uc_b2_count_in_6',\n",
    "                                                                   'uc_b3_count_in_6',\n",
    "                                                                   'uc_b4_count_in_6',\n",
    "                                                                    'uc_b_count_in_6',\n",
    "                                                                   'uc_b1_count_in_3',\n",
    "                                                                   'uc_b2_count_in_3',\n",
    "                                                                   'uc_b3_count_in_3',\n",
    "                                                                   'uc_b4_count_in_3',\n",
    "                                                                    'uc_b_count_in_3',\n",
    "                                                                   'uc_b1_count_in_1',\n",
    "                                                                   'uc_b2_count_in_1',\n",
    "                                                                   'uc_b3_count_in_1',\n",
    "                                                                   'uc_b4_count_in_1',\n",
    "                                                                    'uc_b_count_in_1']].astype(int)\n",
    "\n",
    "# uc_b_count_rank_in_u\n",
    "df_part_2_uc_b_count['uc_b_count_rank_in_u'] = df_part_2_uc_b_count.groupby(['user_id'])['uc_b_count_in_6'].rank(method='min',ascending=False).astype('int')\n",
    "\n",
    "# uc_b_last_time\n",
    "df_part_2.sort_values(by=['user_id','item_category','behavior_type','time'], inplace=True)\n",
    "df_part_2_uc_b_last_time = df_part_2.drop_duplicates(['user_id','item_category','behavior_type'],'last')[['user_id','item_category','behavior_type','time']]\n",
    "\n",
    "df_part_2_uc_b_last_time['uc_b1_last_time'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['behavior_type'] == 1]['time']\n",
    "df_part_2_uc_b_last_time['uc_b2_last_time'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['behavior_type'] == 2]['time']\n",
    "df_part_2_uc_b_last_time['uc_b3_last_time'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['behavior_type'] == 3]['time']\n",
    "df_part_2_uc_b_last_time['uc_b4_last_time'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['behavior_type'] == 4]['time']\n",
    "\n",
    "df_part_2_uc_b_last_time.loc[df_part_2_uc_b_last_time['uc_b1_last_time'].notnull(), 'uc_b1_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_uc_b_last_time['uc_b1_last_time'])             \n",
    "df_part_2_uc_b_last_time['uc_b1_last_hours'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['uc_b1_last_hours'].notnull()]['uc_b1_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_uc_b_last_time.loc[df_part_2_uc_b_last_time['uc_b2_last_time'].notnull(), 'uc_b2_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_uc_b_last_time['uc_b2_last_time'])             \n",
    "df_part_2_uc_b_last_time['uc_b2_last_hours'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['uc_b2_last_hours'].notnull()]['uc_b2_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_uc_b_last_time.loc[df_part_2_uc_b_last_time['uc_b3_last_time'].notnull(), 'uc_b3_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_uc_b_last_time['uc_b3_last_time'])             \n",
    "df_part_2_uc_b_last_time['uc_b3_last_hours'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['uc_b3_last_hours'].notnull()]['uc_b3_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_uc_b_last_time.loc[df_part_2_uc_b_last_time['uc_b4_last_time'].notnull(), 'uc_b4_last_hours'] = (pd.to_datetime('2014-12-05') - df_part_2_uc_b_last_time['uc_b4_last_time'])             \n",
    "df_part_2_uc_b_last_time['uc_b4_last_hours'] = df_part_2_uc_b_last_time[df_part_2_uc_b_last_time['uc_b4_last_hours'].notnull()]['uc_b4_last_hours'].apply(lambda x: x.days*24 + x.seconds//3600)\n",
    "\n",
    "df_part_2_uc_b_last_time = df_part_2_uc_b_last_time[['user_id',\n",
    "                                                     'item_category',\n",
    "                                                     'uc_b1_last_hours',\n",
    "                                                     'uc_b2_last_hours',\n",
    "                                                     'uc_b3_last_hours',\n",
    "                                                     'uc_b4_last_hours']] \n",
    "\n",
    "df_part_2_uc_b_last_time = df_part_2_uc_b_last_time.groupby(['user_id', 'item_category']).agg({'uc_b1_last_hours': np.sum,\n",
    "                                                                                               'uc_b2_last_hours': np.sum,\n",
    "                                                                                               'uc_b3_last_hours': np.sum,\n",
    "                                                                                               'uc_b4_last_hours': np.sum})\n",
    "df_part_2_uc_b_last_time.reset_index(inplace = True)\n",
    "\n",
    "# merge for generation of f_UC_part_2\n",
    "f_UC_part_2 = pd.merge(df_part_2_uc_b_count, df_part_2_uc_b_last_time, how='left', on=['user_id', 'item_category'])\n",
    "\n",
    "# write to csv file\n",
    "f_UC_part_2.to_csv(path_df_part_2_UC, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
