{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### file path\n",
    "### input\n",
    "# data_set keys and lebels\n",
    "path_df_part_1_uic_label = \"mobile/df_part_1_uic_label.csv\"\n",
    "path_df_part_2_uic_label = \"mobile/df_part_2_uic_label.csv\"\n",
    "path_df_part_3_uic       = \"mobile/df_part_3_uic.csv\"\n",
    "\n",
    "# data_set features\n",
    "path_df_part_1_U   = \"mobile/feature/df_part_1_U.csv\"  \n",
    "path_df_part_1_I   = \"mobile/feature/df_part_1_I.csv\"\n",
    "path_df_part_1_C   = \"mobile/feature/df_part_1_C.csv\"\n",
    "path_df_part_1_IC  = \"mobile/feature/df_part_1_IC.csv\"\n",
    "path_df_part_1_UI  = \"mobile/feature/df_part_1_UI.csv\"\n",
    "path_df_part_1_UC  = \"mobile/feature/df_part_1_UC.csv\"\n",
    "\n",
    "path_df_part_2_U   = \"mobile/feature/df_part_2_U.csv\"  \n",
    "path_df_part_2_I   = \"mobile/feature/df_part_2_I.csv\"\n",
    "path_df_part_2_C   = \"mobile/feature/df_part_2_C.csv\"\n",
    "path_df_part_2_IC  = \"mobile/feature/df_part_2_IC.csv\"\n",
    "path_df_part_2_UI  = \"mobile/feature/df_part_2_UI.csv\"\n",
    "path_df_part_2_UC  = \"mobile/feature/df_part_2_UC.csv\"\n",
    "\n",
    "path_df_part_3_U   = \"mobile/feature/df_part_3_U.csv\"  \n",
    "path_df_part_3_I   = \"mobile/feature/df_part_3_I.csv\"\n",
    "path_df_part_3_C   = \"mobile/feature/df_part_3_C.csv\"\n",
    "path_df_part_3_IC  = \"mobile/feature/df_part_3_IC.csv\"\n",
    "path_df_part_3_UI  = \"mobile/feature/df_part_3_UI.csv\"\n",
    "path_df_part_3_UC  = \"mobile/feature/df_part_3_UC.csv\"\n",
    "\n",
    "### out file\n",
    "\n",
    "### intermediate file\n",
    "# data partition with diffferent label\n",
    "path_df_part_2_uic_label_0 = \"mobile/gbdt/k_means_subsample/df_part_2_uic_label_0.csv\"\n",
    "path_df_part_2_uic_label_1 = \"mobile/gbdt/k_means_subsample/df_part_2_uic_label_1.csv\"\n",
    "\n",
    "# training set keys uic-label with k_means clusters' label\n",
    "# path_df_part_1_uic_label_cluster = \"../../data/mobile/gbdt/k_means_subsample/df_part_1_uic_label_cluster.csv\"\n",
    "path_df_part_2_uic_label_cluster = \"mobile/gbdt/k_means_subsample/df_part_2_uic_label_cluster.csv\"\n",
    "\n",
    "# scalers for data standardization store as python pickle\n",
    "# for each part's features\n",
    "# path_df_part_1_scaler = \"../../data/mobile/gbdt/k_means_subsample/df_part_1_scaler\"\n",
    "path_df_part_2_scaler = \"mobile/gbdt/k_means_subsample/df_part_2_scaler\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_read(path, mode = 'r'):\n",
    "    '''the definition of dataframe loading function \n",
    "    '''\n",
    "    path_df = open(path, mode)\n",
    "    try:     df = pd.read_csv(path_df, index_col = False)\n",
    "    finally: path_df.close()\n",
    "    return   df\n",
    "# df_part_1_uic_label = df_read(path_df_part_1_uic_label)  # loading total keys\n",
    "df_part_2_uic_label = df_read(path_df_part_2_uic_label)\n",
    "# df_part_1_uic_label_0 = df_part_1_uic_label[df_part_1_uic_label['label'] == 0]\n",
    "# df_part_1_uic_label_1 = df_part_1_uic_label[df_part_1_uic_label['label'] == 1]\n",
    "df_part_2_uic_label_0 = df_part_2_uic_label[df_part_2_uic_label['label'] == 0]\n",
    "df_part_2_uic_label_1 = df_part_2_uic_label[df_part_2_uic_label['label'] == 1]\n",
    "# df_part_1_uic_label_0.to_csv(path_df_part_1_uic_label_0, index=False)\n",
    "# df_part_1_uic_label_1.to_csv(path_df_part_1_uic_label_1, index=False)\n",
    "df_part_2_uic_label_0.to_csv(path_df_part_2_uic_label_0, index=False)\n",
    "df_part_2_uic_label_1.to_csv(path_df_part_2_uic_label_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "'''Step 2: clustering on negative sub-set\n",
    "    clusters number ~ 35, using mini-batch-k-means\n",
    "'''\n",
    "\n",
    "# clustering based on sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:46: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "chunk 4 done.\n",
      "chunk 5 done.\n",
      "chunk 6 done.\n",
      "chunk 7 done.\n",
      "chunk 8 done.\n",
      "chunk 9 done.\n",
      "chunk 10 done.\n",
      "chunk 11 done.\n",
      "chunk 12 done.\n",
      "chunk 13 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:94: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 done.\n",
      "chunk 2 done.\n",
      "chunk 3 done.\n",
      "chunk 4 done.\n",
      "chunk 5 done.\n",
      "chunk 6 done.\n",
      "chunk 7 done.\n",
      "chunk 8 done.\n",
      "chunk 9 done.\n",
      "chunk 10 done.\n",
      "chunk 11 done.\n",
      "chunk 12 done.\n",
      "chunk 13 done.\n",
      "chunk 14 done.\n",
      "chunk 15 done.\n",
      "chunk 16 done.\n",
      "chunk 17 done.\n",
      "chunk 18 done.\n",
      "chunk 19 done.\n",
      "chunk 20 done.\n",
      "chunk 21 done.\n",
      "chunk 22 done.\n",
      "chunk 23 done.\n",
      "chunk 24 done.\n",
      "chunk 25 done.\n",
      "chunk 26 done.\n",
      "chunk 27 done.\n",
      "chunk 28 done.\n",
      "chunk 29 done.\n",
      "chunk 30 done.\n",
      "chunk 31 done.\n",
      "chunk 32 done.\n",
      "chunk 33 done.\n",
      "chunk 34 done.\n",
      "chunk 35 done.\n",
      "chunk 36 done.\n",
      "chunk 37 done.\n",
      "chunk 38 done.\n",
      "chunk 39 done.\n",
      "chunk 40 done.\n",
      "chunk 41 done.\n",
      "chunk 42 done.\n",
      "chunk 43 done.\n",
      "chunk 44 done.\n",
      "chunk 45 done.\n",
      "chunk 46 done.\n",
      "chunk 47 done.\n",
      "chunk 48 done.\n",
      "chunk 49 done.\n",
      "chunk 50 done.\n",
      "chunk 51 done.\n",
      "chunk 52 done.\n",
      "chunk 53 done.\n",
      "chunk 54 done.\n",
      "chunk 55 done.\n",
      "chunk 56 done.\n",
      "chunk 57 done.\n",
      "chunk 58 done.\n",
      "chunk 59 done.\n",
      "chunk 60 done.\n",
      "chunk 61 done.\n",
      "chunk 62 done.\n",
      "chunk 63 done.\n",
      "chunk 64 done.\n",
      "chunk 65 done.\n",
      "chunk 66 done.\n",
      "chunk 67 done.\n",
      "chunk 68 done.\n",
      "chunk 69 done.\n",
      "chunk 70 done.\n",
      "chunk 71 done.\n",
      "chunk 72 done.\n",
      "chunk 73 done.\n",
      "chunk 74 done.\n",
      "chunk 75 done.\n",
      "chunk 76 done.\n",
      "chunk 77 done.\n",
      "chunk 78 done.\n",
      "chunk 79 done.\n",
      "chunk 80 done.\n",
      "chunk 81 done.\n",
      "chunk 82 done.\n",
      "chunk 83 done.\n",
      "chunk 84 done.\n",
      "chunk 85 done.\n",
      "chunk 86 done.\n",
      "chunk 87 done.\n",
      "chunk 88 done.\n",
      "chunk 89 done.\n",
      "chunk 90 done.\n",
      "chunk 91 done.\n",
      "chunk 92 done.\n",
      "chunk 93 done.\n",
      "chunk 94 done.\n",
      "chunk 95 done.\n",
      "chunk 96 done.\n",
      "chunk 97 done.\n",
      "chunk 98 done.\n",
      "chunk 99 done.\n",
      "chunk 100 done.\n",
      "chunk 101 done.\n",
      "chunk 102 done.\n",
      "chunk 103 done.\n",
      "chunk 104 done.\n",
      "chunk 105 done.\n",
      "chunk 106 done.\n",
      "chunk 107 done.\n",
      "chunk 108 done.\n",
      "chunk 109 done.\n",
      "chunk 110 done.\n",
      "chunk 111 done.\n",
      "chunk 112 done.\n",
      "chunk 113 done.\n",
      "chunk 114 done.\n",
      "chunk 115 done.\n",
      "chunk 116 done.\n",
      "chunk 117 done.\n",
      "chunk 118 done.\n",
      "chunk 119 done.\n",
      "chunk 120 done.\n",
      "chunk 121 done.\n",
      "chunk 122 done.\n"
     ]
    }
   ],
   "source": [
    "##### part_2 #####\n",
    "# loading features\n",
    "df_part_2_U  = df_read(path_df_part_2_U )   \n",
    "df_part_2_I  = df_read(path_df_part_2_I )\n",
    "df_part_2_C  = df_read(path_df_part_2_C )\n",
    "df_part_2_IC = df_read(path_df_part_2_IC)\n",
    "df_part_2_UI = df_read(path_df_part_2_UI)\n",
    "df_part_2_UC = df_read(path_df_part_2_UC)\n",
    "\n",
    "# process by chunk as ui-pairs size is too big\n",
    "\n",
    "# for get scale transform mechanism to large scale of data\n",
    "scaler_2 = preprocessing.StandardScaler()\n",
    "batch = 0\n",
    "for df_part_2_uic_label_0 in pd.read_csv(open(path_df_part_2_uic_label_0, 'r'), chunksize=150000): \n",
    "    try:\n",
    "        # construct of part_1's sub-training set\n",
    "        train_data_df_part_2 = pd.merge(df_part_2_uic_label_0, df_part_2_U, how='left', on=['user_id'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_I,  how='left', on=['item_id'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_C,  how='left', on=['item_category'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_IC, how='left', on=['item_id','item_category'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_UI, how='left', on=['user_id','item_id','item_category','label'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_UC, how='left', on=['user_id','item_category'])\n",
    "\n",
    "        train_X_2 = train_data_df_part_2.as_matrix(['u_b1_count_in_6','u_b2_count_in_6','u_b3_count_in_6','u_b4_count_in_6','u_b_count_in_6', \n",
    "                                                    'u_b1_count_in_3','u_b2_count_in_3','u_b3_count_in_3','u_b4_count_in_3','u_b_count_in_3', \n",
    "                                                    'u_b1_count_in_1','u_b2_count_in_1','u_b3_count_in_1','u_b4_count_in_1','u_b_count_in_1', \n",
    "                                                    'u_b4_rate',\n",
    "                                                    'i_u_count_in_6','i_u_count_in_3','i_u_count_in_1',\n",
    "                                                    'i_b1_count_in_6','i_b2_count_in_6','i_b3_count_in_6','i_b4_count_in_6','i_b_count_in_6', \n",
    "                                                    'i_b1_count_in_3','i_b2_count_in_3','i_b3_count_in_3','i_b4_count_in_3','i_b_count_in_3',\n",
    "                                                    'i_b1_count_in_1','i_b2_count_in_1','i_b3_count_in_1','i_b4_count_in_1','i_b_count_in_1', \n",
    "                                                    'i_b4_rate',\n",
    "                                                    'c_b1_count_in_6','c_b2_count_in_6','c_b3_count_in_6','c_b4_count_in_6','c_b_count_in_6',\n",
    "                                                    'c_b1_count_in_3','c_b2_count_in_3','c_b3_count_in_3','c_b4_count_in_3','c_b_count_in_3',\n",
    "                                                    'c_b1_count_in_1','c_b2_count_in_1','c_b3_count_in_1','c_b4_count_in_1','c_b_count_in_1',\n",
    "                                                    'c_b4_rate',\n",
    "                                                    'ic_u_rank_in_c','ic_b_rank_in_c','ic_b4_rank_in_c', \n",
    "                                                    'ui_b1_count_in_6','ui_b2_count_in_6','ui_b3_count_in_6','ui_b4_count_in_6','ui_b_count_in_6',\n",
    "                                                    'ui_b1_count_in_3','ui_b2_count_in_3','ui_b3_count_in_3','ui_b4_count_in_3','ui_b_count_in_3',\n",
    "                                                    'ui_b1_count_in_1','ui_b2_count_in_1','ui_b3_count_in_1','ui_b4_count_in_1','ui_b_count_in_1', \n",
    "                                                    'ui_b_count_rank_in_u','ui_b_count_rank_in_uc',\n",
    "                                                    'uc_b1_count_in_6','uc_b2_count_in_6','uc_b3_count_in_6','uc_b4_count_in_6','uc_b_count_in_6', \n",
    "                                                    'uc_b1_count_in_3','uc_b2_count_in_3','uc_b3_count_in_3','uc_b4_count_in_3','uc_b_count_in_3', \n",
    "                                                    'uc_b1_count_in_1','uc_b2_count_in_1','uc_b3_count_in_1','uc_b4_count_in_1','uc_b_count_in_1',\n",
    "                                                    'uc_b_count_rank_in_u'])\n",
    "        # fit the scaler\n",
    "        scaler_2.partial_fit(train_X_2)\n",
    "        \n",
    "        batch += 1\n",
    "        print('chunk %d done.' %batch) \n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\"finish.\")\n",
    "        break \n",
    "\n",
    "# initial clusters\n",
    "mbk_2 = MiniBatchKMeans(init='k-means++', n_clusters=1000, batch_size=500, reassignment_ratio=10**-4)  \n",
    "\n",
    "# process by chunk as ui-pairs size is too big\n",
    "batch = 0\n",
    "classes_2 = []\n",
    "for df_part_2_uic_label_0 in pd.read_csv(open(path_df_part_2_uic_label_0, 'r'), chunksize=15000): \n",
    "    try:\n",
    "        # construct of part_1's sub-training set\n",
    "        train_data_df_part_2 = pd.merge(df_part_2_uic_label_0, df_part_2_U, how='left', on=['user_id'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_I,  how='left', on=['item_id'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_C,  how='left', on=['item_category'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_IC, how='left', on=['item_id','item_category'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_UI, how='left', on=['user_id','item_id','item_category','label'])\n",
    "        train_data_df_part_2 = pd.merge(train_data_df_part_2, df_part_2_UC, how='left', on=['user_id','item_category'])\n",
    "        \n",
    "        train_X_2 = train_data_df_part_2.as_matrix(['u_b1_count_in_6','u_b2_count_in_6','u_b3_count_in_6','u_b4_count_in_6','u_b_count_in_6', \n",
    "                                                    'u_b1_count_in_3','u_b2_count_in_3','u_b3_count_in_3','u_b4_count_in_3','u_b_count_in_3', \n",
    "                                                    'u_b1_count_in_1','u_b2_count_in_1','u_b3_count_in_1','u_b4_count_in_1','u_b_count_in_1', \n",
    "                                                    'u_b4_rate',\n",
    "                                                    'i_u_count_in_6','i_u_count_in_3','i_u_count_in_1',\n",
    "                                                    'i_b1_count_in_6','i_b2_count_in_6','i_b3_count_in_6','i_b4_count_in_6','i_b_count_in_6', \n",
    "                                                    'i_b1_count_in_3','i_b2_count_in_3','i_b3_count_in_3','i_b4_count_in_3','i_b_count_in_3',\n",
    "                                                    'i_b1_count_in_1','i_b2_count_in_1','i_b3_count_in_1','i_b4_count_in_1','i_b_count_in_1', \n",
    "                                                    'i_b4_rate',\n",
    "                                                    'c_b1_count_in_6','c_b2_count_in_6','c_b3_count_in_6','c_b4_count_in_6','c_b_count_in_6',\n",
    "                                                    'c_b1_count_in_3','c_b2_count_in_3','c_b3_count_in_3','c_b4_count_in_3','c_b_count_in_3',\n",
    "                                                    'c_b1_count_in_1','c_b2_count_in_1','c_b3_count_in_1','c_b4_count_in_1','c_b_count_in_1',\n",
    "                                                    'c_b4_rate',\n",
    "                                                    'ic_u_rank_in_c','ic_b_rank_in_c','ic_b4_rank_in_c', \n",
    "                                                    'ui_b1_count_in_6','ui_b2_count_in_6','ui_b3_count_in_6','ui_b4_count_in_6','ui_b_count_in_6',\n",
    "                                                    'ui_b1_count_in_3','ui_b2_count_in_3','ui_b3_count_in_3','ui_b4_count_in_3','ui_b_count_in_3',\n",
    "                                                    'ui_b1_count_in_1','ui_b2_count_in_1','ui_b3_count_in_1','ui_b4_count_in_1','ui_b_count_in_1', \n",
    "                                                    'ui_b_count_rank_in_u','ui_b_count_rank_in_uc',\n",
    "                                                    'uc_b1_count_in_6','uc_b2_count_in_6','uc_b3_count_in_6','uc_b4_count_in_6','uc_b_count_in_6', \n",
    "                                                    'uc_b1_count_in_3','uc_b2_count_in_3','uc_b3_count_in_3','uc_b4_count_in_3','uc_b_count_in_3', \n",
    "                                                    'uc_b1_count_in_1','uc_b2_count_in_1','uc_b3_count_in_1','uc_b4_count_in_1','uc_b_count_in_1',\n",
    "                                                    'uc_b_count_rank_in_u'])\n",
    "        # feature standardization\n",
    "        standardized_train_X_2 = scaler_2.transform(train_X_2)\n",
    "        \n",
    "        # fit clustering model\n",
    "        mbk_2.partial_fit(standardized_train_X_2)\n",
    "        classes_2 = np.append(classes_2, mbk_2.labels_)\n",
    "        \n",
    "        batch += 1\n",
    "        print('chunk %d done.' %batch) \n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\" ------------ k-means finished on part 2 ------------.\")\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler_2, open(path_df_part_2_scaler,'wb'))\n",
    "# add a new attr for keys\n",
    "# df_part_1_uic_label_0 = df_read(path_df_part_1_uic_label_0)\n",
    "# df_part_1_uic_label_1 = df_read(path_df_part_1_uic_label_1)\n",
    "df_part_2_uic_label_0 = df_read(path_df_part_2_uic_label_0)\n",
    "df_part_2_uic_label_1 = df_read(path_df_part_2_uic_label_1)\n",
    "    \n",
    "# df_part_1_uic_label_0['class'] = classes_1.astype('int') + 1\n",
    "# df_part_1_uic_label_1['class'] = 0\n",
    "df_part_2_uic_label_0['class'] = classes_2.astype('int') + 1\n",
    "df_part_2_uic_label_1['class'] = 0\n",
    "\n",
    "# df_part_1_uic_label_class = pd.concat([df_part_1_uic_label_0, df_part_1_uic_label_1])\n",
    "df_part_2_uic_label_class = pd.concat([df_part_2_uic_label_0, df_part_2_uic_label_1])\n",
    "   \n",
    "# df_part_1_uic_label_class.to_csv(path_df_part_1_uic_label_cluster, index=False)\n",
    "df_part_2_uic_label_class.to_csv(path_df_part_2_uic_label_cluster, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
